{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Or-Gindes/MachineLearningAssignment3/blob/master/Assignment3_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qOhjmU2gPsr"
      },
      "source": [
        "## Machine Learning - Assignment No. 3\n",
        "\n",
        "### Part 2 â€“ Aim: Practice the usage of CNN (Convolutional Neural Network). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqakNzcUggB_"
      },
      "source": [
        "In this part you will use at least two pretrained CNNs to identify the type of a flower that appears in an image. You will need to choose your pretrained models and use Transfer Learning to associate flower images into their corresponding categories.\n",
        "\n",
        "We would like the model to classify the image into its category (the dandelions category in this example). The model should be probabilistic and returns the probability of a flower belonging to each of the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1iYZ1aRr-rw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b26bef-8367-472e-b2c9-2c423c4f59ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('imagelabels.mat', <http.client.HTTPMessage at 0x7fef957bb850>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Get /flowers/102 files\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "seed=123\n",
        "\n",
        "dataset = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
        "# segs = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz\"\n",
        "labels = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
        "urlretrieve(dataset, dataset.rsplit('/', 1)[-1])\n",
        "# urlretrieve(segs, segs.rsplit('/', 1)[-1])\n",
        "urlretrieve(labels, labels.rsplit('/', 1)[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "62damTry_-hY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "b6a67f38-4840-4f74-ddd5-8357c1304d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes: 102\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHTCAYAAADBKdkpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7MElEQVR4nO3dd3gU5f7+8XsT0ihJDCUhGEIVCF16UdAgoSoCR5o0EY6a0KI0ERBEUJQiiHD0SFFBRUD0gNLBgqEFqdKCNA0JSEmogZD5/eGP/bomQTbZZTfD+3Vdc13MzLMznxmC3nnmeWYthmEYAgAAMCkPVxcAAADgTIQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAFbHjh2TxWLR22+/7bBjbty4URaLRRs3bnTYMW959dVXZbFYHH7crDRt2lRNmza1rt+6rsWLF9+V8/fq1UulSpW6K+cCzIawA+Rx8+bNk8Vi0fbt211dSq7cuo5bi6+vr0JDQxUVFaXp06fr4sWLDjlPYmKiXn31Ve3cudMhx3Mkd64NyMsIOwDcyrhx4/Txxx9r1qxZ6t+/vyRp0KBBqlq1qnbv3m3T9pVXXtHVq1ftOn5iYqLGjh1rd6BYvXq1Vq9ebddn7HW72j744AMdPHjQqecHzCqfqwsAgL9q2bKlateubV0fMWKE1q9frzZt2ujxxx/X/v375efnJ0nKly+f8uVz7n/Grly5ovz588vb29up5/knXl5eLj0/kJfRswPcA65fv67Ro0erVq1aCggIUIECBfTQQw9pw4YN2X5m6tSpCg8Pl5+fn5o0aaK9e/dmanPgwAF17NhRQUFB8vX1Ve3atfX11187vP5HH31Uo0aN0vHjx/XJJ59Yt2c1ZmfNmjVq3LixAgMDVbBgQVWoUEEvv/yypD/H2dSpU0eS1Lt3b+sjs3nz5kn6c1xOlSpVFB8fr4cfflj58+e3fvbvY3ZuuXnzpl5++WWFhISoQIECevzxx3Xy5EmbNqVKlVKvXr0yffavx/yn2rIas3P58mW9+OKLCgsLk4+PjypUqKC3335bhmHYtLNYLIqJidGyZctUpUoV+fj4qHLlylq5cmXWNxwwGXp2gHtAamqq/vvf/6pLly7q27evLl68qA8//FBRUVHaunWratSoYdP+o48+0sWLFxUdHa1r167pnXfe0aOPPqo9e/YoODhYkrRv3z41atRIJUqU0PDhw1WgQAEtWrRI7dq105IlS/Tkk0869Bq6d++ul19+WatXr1bfvn2zbLNv3z61adNG1apV07hx4+Tj46OEhARt2rRJklSpUiWNGzdOo0ePVr9+/fTQQw9Jkho2bGg9xtmzZ9WyZUt17txZTz/9tPV6s/P666/LYrFo2LBhOn36tKZNm6ZmzZpp586d1h6oO3Entf2VYRh6/PHHtWHDBvXp00c1atTQqlWrNGTIEP3++++aOnWqTfsff/xRS5cu1QsvvKBChQpp+vTp6tChg06cOKHChQvfcZ1AnmQAyNPmzp1rSDK2bduWbZv09HQjLS3NZtv58+eN4OBg45lnnrFuO3r0qCHJ8PPzM3777Tfr9i1bthiSjMGDB1u3RUZGGlWrVjWuXbtm3ZaRkWE0bNjQKF++vHXbhg0bDEnGhg0bcn0dAQEBRs2aNa3rY8aMMf76n7GpU6cakowzZ85ke4xt27YZkoy5c+dm2tekSRNDkjF79uws9zVp0iTTdZUoUcJITU21bl+0aJEhyXjnnXes28LDw42ePXv+4zFvV1vPnj2N8PBw6/qyZcsMScb48eNt2nXs2NGwWCxGQkKCdZskw9vb22bbrl27DEnGjBkzMp0LMBseYwH3AE9PT+uYk4yMDJ07d07p6emqXbu2duzYkal9u3btVKJECet63bp1Va9ePX3zzTeSpHPnzmn9+vV66qmndPHiRf3xxx/6448/dPbsWUVFRenw4cP6/fffHX4dBQsWvO2srMDAQEnSV199pYyMjBydw8fHR717977j9j169FChQoWs6x07dlTx4sWt98pZvvnmG3l6emrAgAE221988UUZhqFvv/3WZnuzZs1UtmxZ63q1atXk7++vX3/91al1Au6AsAPcI+bPn69q1arJ19dXhQsXVtGiRbVixQqlpKRkalu+fPlM2x544AEdO3ZMkpSQkCDDMDRq1CgVLVrUZhkzZowk6fTp0w6/hkuXLtkEi7/r1KmTGjVqpGeffVbBwcHq3LmzFi1aZFfwKVGihF2Dkf9+rywWi8qVK2e9V85y/PhxhYaGZroflSpVsu7/q5IlS2Y6xn333afz5887r0jATTBmB7gHfPLJJ+rVq5fatWunIUOGqFixYvL09NTEiRN15MgRu493Kzy89NJLioqKyrJNuXLlclXz3/32229KSUm57XH9/Pz0/fffa8OGDVqxYoVWrlypzz//XI8++qhWr14tT0/PfzyPPeNs7lR2Lz68efPmHdXkCNmdx/jbYGbAjAg7wD1g8eLFKlOmjJYuXWrzP95bvTB/d/jw4UzbDh06ZJ0NVKZMGUl/Todu1qyZ4wvOwscffyxJ2YarWzw8PBQZGanIyEhNmTJFEyZM0MiRI7VhwwY1a9bM4W9c/vu9MgxDCQkJqlatmnXbfffdpwsXLmT67PHjx633Uso+FGUlPDxca9eu1cWLF216dw4cOGDdD+BPPMYC7gG3fqv/62/xW7ZsUVxcXJbtly1bZjPmZuvWrdqyZYtatmwpSSpWrJiaNm2q//znPzp16lSmz585c8aR5Wv9+vV67bXXVLp0aXXr1i3bdufOncu07dZMs7S0NElSgQIFJCnL8JETt2au3bJ48WKdOnXKeq8kqWzZstq8ebOuX79u3bZ8+fJMU9Ttqa1Vq1a6efOm3n33XZvtU6dOlcVisTk/cK+jZwcwiTlz5mT53pSBAweqTZs2Wrp0qZ588km1bt1aR48e1ezZsxUREaFLly5l+ky5cuXUuHFjPf/880pLS9O0adNUuHBhDR061Npm5syZaty4sapWraq+ffuqTJkySk5OVlxcnH777Tft2rUrR9fx7bff6sCBA0pPT1dycrLWr1+vNWvWKDw8XF9//bV8fX2z/ey4ceP0/fffq3Xr1goPD9fp06f13nvv6f7771fjxo0l/Rk8AgMDNXv2bBUqVEgFChRQvXr1VLp06RzVGxQUpMaNG6t3795KTk7WtGnTVK5cOZvp8c8++6wWL16sFi1a6KmnntKRI0f0ySef2AwYtre2tm3b6pFHHtHIkSN17NgxVa9eXatXr9ZXX32lQYMGZTo2cE9z6VwwALl2a8p2dsvJkyeNjIwMY8KECUZ4eLjh4+Nj1KxZ01i+fHmm6cy3pp6/9dZbxuTJk42wsDDDx8fHeOihh4xdu3ZlOveRI0eMHj16GCEhIYaXl5dRokQJo02bNsbixYutbeyden5r8fb2NkJCQozHHnvMeOedd2ymd9/y96nn69atM5544gkjNDTU8Pb2NkJDQ40uXboYhw4dsvncV199ZURERBj58uWzmerdpEkTo3LlylnWl93U808//dQYMWKEUaxYMcPPz89o3bq1cfz48Uyfnzx5slGiRAnDx8fHaNSokbF9+/ZMx7xdbX//uzIMw7h48aIxePBgIzQ01PDy8jLKly9vvPXWW0ZGRoZNO0lGdHR0ppqymxIPmI3FMBidBgAAzIsxOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNR4qaD+/J6fxMREFSpUyOGvkgcAAM5hGIYuXryo0NBQeXhk339D2JGUmJiosLAwV5cBAABy4OTJk7r//vuz3U/Ykaxfonfy5En5+/u7uBoAAHAnUlNTFRYWZvNluFkh7Oj/vmnY39+fsAMAQB7zT0NQGKAMAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMLZ+rCwAAIDulhq+w/vnYG61dWAnyMnp2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqbk07EycOFF16tRRoUKFVKxYMbVr104HDx60adO0aVNZLBab5bnnnrNpc+LECbVu3Vr58+dXsWLFNGTIEKWnp9/NSwEAAG7Kpe/Z+e677xQdHa06deooPT1dL7/8spo3b65ffvlFBQoUsLbr27evxo0bZ13Pnz+/9c83b95U69atFRISop9++kmnTp1Sjx495OXlpQkTJtzV6wEAAO7HpWFn5cqVNuvz5s1TsWLFFB8fr4cffti6PX/+/AoJCcnyGKtXr9Yvv/yitWvXKjg4WDVq1NBrr72mYcOG6dVXX5W3t7dTrwEAALg3txqzk5KSIkkKCgqy2b5gwQIVKVJEVapU0YgRI3TlyhXrvri4OFWtWlXBwcHWbVFRUUpNTdW+ffuyPE9aWppSU1NtFgCA45QavsJmAVzJbb4uIiMjQ4MGDVKjRo1UpUoV6/auXbsqPDxcoaGh2r17t4YNG6aDBw9q6dKlkqSkpCSboCPJup6UlJTluSZOnKixY8c66UoAAIA7cZuwEx0drb179+rHH3+02d6vXz/rn6tWrarixYsrMjJSR44cUdmyZXN0rhEjRig2Nta6npqaqrCwsJwVDgAA3JpbPMaKiYnR8uXLtWHDBt1///23bVuvXj1JUkJCgiQpJCREycnJNm1urWc3zsfHx0f+/v42CwAAMCeXhh3DMBQTE6Mvv/xS69evV+nSpf/xMzt37pQkFS9eXJLUoEED7dmzR6dPn7a2WbNmjfz9/RUREeGUugEAQN7h0sdY0dHRWrhwob766isVKlTIOsYmICBAfn5+OnLkiBYuXKhWrVqpcOHC2r17twYPHqyHH35Y1apVkyQ1b95cERER6t69uyZNmqSkpCS98sorio6Olo+PjysvDwAAuAGX9uzMmjVLKSkpatq0qYoXL25dPv/8c0mSt7e31q5dq+bNm6tixYp68cUX1aFDB/3vf/+zHsPT01PLly+Xp6enGjRooKefflo9evSweS8PAAC4d7m0Z8cwjNvuDwsL03ffffePxwkPD9c333zjqLIAAICJuMUAZQAAAGch7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFNzadiZOHGi6tSpo0KFCqlYsWJq166dDh48aNPm2rVrio6OVuHChVWwYEF16NBBycnJNm1OnDih1q1bK3/+/CpWrJiGDBmi9PT0u3kpAADATbk07Hz33XeKjo7W5s2btWbNGt24cUPNmzfX5cuXrW0GDx6s//3vf/riiy/03XffKTExUe3bt7fuv3nzplq3bq3r16/rp59+0vz58zVv3jyNHj3aFZcEAADcjMUwDMPVRdxy5swZFStWTN99950efvhhpaSkqGjRolq4cKE6duwoSTpw4IAqVaqkuLg41a9fX99++63atGmjxMREBQcHS5Jmz56tYcOG6cyZM/L29s50nrS0NKWlpVnXU1NTFRYWppSUFPn7+9+diwUAEys1fIXN+rE3Wuf6ODk9BswrNTVVAQEB//j/b7cas5OSkiJJCgoKkiTFx8frxo0batasmbVNxYoVVbJkScXFxUmS4uLiVLVqVWvQkaSoqCilpqZq3759WZ5n4sSJCggIsC5hYWHOuiQAAOBibhN2MjIyNGjQIDVq1EhVqlSRJCUlJcnb21uBgYE2bYODg5WUlGRt89egc2v/rX1ZGTFihFJSUqzLyZMnHXw1AADAXeRzdQG3REdHa+/evfrxxx+dfi4fHx/5+Pg4/TwAAMD13KJnJyYmRsuXL9eGDRt0//33W7eHhITo+vXrunDhgk375ORkhYSEWNv8fXbWrfVbbQAAwL3LpWHHMAzFxMToyy+/1Pr161W6dGmb/bVq1ZKXl5fWrVtn3Xbw4EGdOHFCDRo0kCQ1aNBAe/bs0enTp61t1qxZI39/f0VERNydCwEAAG7LpY+xoqOjtXDhQn311VcqVKiQdYxNQECA/Pz8FBAQoD59+ig2NlZBQUHy9/dX//791aBBA9WvX1+S1Lx5c0VERKh79+6aNGmSkpKS9Morryg6OppHVQAAwLVhZ9asWZKkpk2b2myfO3euevXqJUmaOnWqPDw81KFDB6WlpSkqKkrvvfeeta2np6eWL1+u559/Xg0aNFCBAgXUs2dPjRs37m5dBgAA2WL6vOu5NOzcySt+fH19NXPmTM2cOTPbNuHh4frmm28cWRoAADAJtxigDAAA4CxuM/UcAIB7gaPeLo07R88OAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNb4IFABgen/98k2+ePPeQ88OAAAwNcIOAAAwNcIOAAAwNbvDzvz587Vixf89+xw6dKgCAwPVsGFDHT9+3KHFAQAA5JbdYWfChAny8/OTJMXFxWnmzJmaNGmSihQposGDBzu8QAAAgNywezbWyZMnVa5cOUnSsmXL1KFDB/Xr10+NGjVS06ZNHV0fAABArtjds1OwYEGdPXtWkrR69Wo99thjkiRfX19dvXrVsdUBAADkkt09O4899pieffZZ1axZU4cOHVKrVq0kSfv27VOpUqUcXR8AAECu2N2zM3PmTDVo0EBnzpzRkiVLVLhwYUlSfHy8unTp4vACAQAAcsPunp3AwEC9++67mbaPHTvWIQUBAAA4Uo7es/PDDz/o6aefVsOGDfX7779Lkj7++GP9+OOPDi0OAAAgt+wOO0uWLFFUVJT8/Py0Y8cOpaWlSZJSUlI0YcIEhxcIAACQG3aHnfHjx2v27Nn64IMP5OXlZd3eqFEj7dixw6HFAQAA5JbdYefgwYN6+OGHM20PCAjQhQsXHFETAACAw9gddkJCQpSQkJBp+48//qgyZco4pCgAAABHsTvs9O3bVwMHDtSWLVtksViUmJioBQsW6KWXXtLzzz/vjBoBAAByzO6p58OHD1dGRoYiIyN15coVPfzww/Lx8dFLL72k/v37O6NGAACAHLM77FgsFo0cOVJDhgxRQkKCLl26pIiICBUsWNAZ9QEwsVLDV1j/fOyN1i6sBICZ2R12bvH29lZERIQjawEAAHA4u8POk08+KYvFkmm7xWKRr6+vypUrp65du6pChQoOKRAAACA37B6gHBAQoPXr12vHjh2yWCyyWCz6+eeftX79eqWnp+vzzz9X9erVtWnTJmfUCwAAYBe7e3ZCQkLUtWtXvfvuu/Lw+DMrZWRkaODAgSpUqJA+++wzPffccxo2bBhfHwEAAFzO7p6dDz/8UIMGDbIGHUny8PBQ//799f7778tisSgmJkZ79+51aKEAAAA5YXfYSU9P14EDBzJtP3DggG7evClJ8vX1zXJcDwAAwN1m92Os7t27q0+fPnr55ZdVp04dSdK2bds0YcIE9ejRQ5L03XffqXLlyo6tFACQp/GqAbiK3WFn6tSpCg4O1qRJk5ScnCxJCg4O1uDBgzVs2DBJUvPmzdWiRQvHVgoAAJADdocdT09PjRw5UiNHjlRqaqokyd/f36ZNyZIlHVMdAABALuX4pYJS5pADAADgbnIUdhYvXqxFixbpxIkTun79us2+HTt2OKQwAAAAR7B7Ntb06dPVu3dvBQcH6+eff1bdunVVuHBh/frrr2rZsqUzagQAu5QavsK6AIDdYee9997T+++/rxkzZsjb21tDhw7VmjVrNGDAAKWkpDijRgAAgByzO+ycOHFCDRs2lCT5+fnp4sWLkv6ckv7pp586tjoAAIBcsjvshISE6Ny5c5L+nHW1efNmSdLRo0dlGIZjqwMAAMglu8POo48+qq+//lqS1Lt3bw0ePFiPPfaYOnXqpCeffNLhBQIAAOSG3bOx3n//fWVkZEiSoqOjVbhwYf300096/PHH9e9//9vhBQIAAOSG3WHHw8PD5ktAO3furM6dOzu0KAAAAEfJ0Xt2rl27pt27d+v06dPWXp5bHn/8cYcUBgAA4Ah2h52VK1eqR48e+uOPPzLts1gs1m8+BwAAcAd2D1Du37+//vWvf+nUqVPKyMiwWQg6AADA3dgddpKTkxUbG6vg4GBn1AMAAOBQdj/G6tixozZu3KiyZcs6ox4AJvHXr2o49kZrF1YC4F5nd9h599139a9//Us//PCDqlatKi8vL5v9AwYMcFhxAAAAuWX3Y6xPP/1Uq1ev1pIlSzRjxgxNnTrVukybNs2uY33//fdq27atQkNDZbFYtGzZMpv9vXr1ksVisVlatGhh0+bcuXPq1q2b/P39FRgYqD59+ujSpUv2XhYAADApu3t2Ro4cqbFjx2r48OE279vJicuXL6t69ep65pln1L59+yzbtGjRQnPnzrWu+/j42Ozv1q2bTp06pTVr1ujGjRvq3bu3+vXrp4ULF+aqNgAAYA52h53r16+rU6dOuQ46ktSyZUu1bNnytm18fHwUEhKS5b79+/dr5cqV2rZtm2rXri1JmjFjhlq1aqW3335boaGhua4RAADkbXYnlp49e+rzzz93Ri1Z2rhxo4oVK6YKFSro+eef19mzZ6374uLiFBgYaA06ktSsWTN5eHhoy5Yt2R4zLS1NqampNgsAADAnu3t2bt68qUmTJmnVqlWqVq1apgHKU6ZMcVhxLVq0UPv27VW6dGkdOXJEL7/8slq2bKm4uDh5enoqKSlJxYoVs/lMvnz5FBQUpKSkpGyPO3HiRI0dO9ZhdQIAAPdld9jZs2ePatasKUnau3evzT6LxeKYqv6/v37nVtWqVVWtWjWVLVtWGzduVGRkZI6PO2LECMXGxlrXU1NTFRYWlqtaAQCAe7I77GzYsMEZddyRMmXKqEiRIkpISFBkZKRCQkJ0+vRpmzbp6ek6d+5ctuN8pD/HAf19oDMAADCn3I8yvot+++03nT17VsWLF5ckNWjQQBcuXFB8fLy1zfr165WRkaF69eq5qkwAAOBG7rhnJ7up4X+3dOnSOz75pUuXlJCQYF0/evSodu7cqaCgIAUFBWns2LHq0KGDQkJCdOTIEQ0dOlTlypVTVFSUJKlSpUpq0aKF+vbtq9mzZ+vGjRuKiYlR586dmYkFAAAk2RF2AgICHH7y7du365FHHrGu3xpH07NnT82aNUu7d+/W/PnzdeHCBYWGhqp58+Z67bXXbB5BLViwQDExMYqMjJSHh4c6dOig6dOnO7xWAACQN91x2Pnri/0cpWnTpjIMI9v9q1at+sdjBAUF8QJBAACQrTw1ZgcAAMBehB0AAGBqhB0AAGBqhB0AAGBqdxR2HnzwQZ0/f16SNG7cOF25csWpRQEAADjKHYWd/fv36/Lly5KksWPH6tKlS04tCgAAwFHuaOp5jRo11Lt3bzVu3FiGYejtt99WwYIFs2w7evRohxYIAACQG3cUdubNm6cxY8Zo+fLlslgs+vbbb5UvX+aPWiwWwg4AAHArdxR2KlSooM8++0yS5OHhoXXr1qlYsWJOLQwAAMAR7P7W84yMDGfUAQAA4BR2hx1JOnLkiKZNm6b9+/dLkiIiIjRw4ECVLVvWocUBAOAMpYavsFk/9kZrF1WCu8Hu9+ysWrVKERER2rp1q6pVq6Zq1appy5Ytqly5stasWeOMGgEAAHLM7p6d4cOHa/DgwXrjjTcybR82bJgee+wxhxUHAACQW3b37Ozfv199+vTJtP2ZZ57RL7/84pCiAAAAHMXusFO0aFHt3Lkz0/adO3cyQwsAALgdux9j9e3bV/369dOvv/6qhg0bSpI2bdqkN998U7GxsQ4vEAAAIDfsDjujRo1SoUKFNHnyZI0YMUKSFBoaqldffVUDBgxweIEAAAC5YXfYsVgsGjx4sAYPHqyLFy9KkgoVKuTwwgAAABwhR+/ZuYWQAwAA3F2uwg4AAGbASwbNjbADAMgzCCXICbunngMAAOQldoWdGzduKDIyUocPH3ZWPQAAAA5l12MsLy8v7d6921m1AACQazzqwt/ZPWbn6aef1ocffpjpu7EAALAHoQR3i91hJz09XXPmzNHatWtVq1YtFShQwGb/lClTHFYcAABAbtkddvbu3asHH3xQknTo0CGbfRaLxTFVAQAAOIjdYWfDhg3OqAMAAMApcjz1PCEhQatWrdLVq1clSYZhOKwoAAAAR7E77Jw9e1aRkZF64IEH1KpVK506dUqS1KdPH7344osOLxAAACA37A47gwcPlpeXl06cOKH8+fNbt3fq1EkrV650aHEAAAC5ZfeYndWrV2vVqlW6//77bbaXL19ex48fd1hhAAAAjmB3z87ly5dtenRuOXfunHx8fBxSFAAAgKPYHXYeeughffTRR9Z1i8WijIwMTZo0SY888ohDiwMAAMgtux9jTZo0SZGRkdq+fbuuX7+uoUOHat++fTp37pw2bdrkjBoBuBBvuQWQ19nds1OlShUdOnRIjRs31hNPPKHLly+rffv2+vnnn1W2bFln1AgAAJBjdvfsSFJAQIBGjhzp6FoAAAAcLkdh5/z58/rwww+1f/9+SVJERIR69+6toKAghxYHAACQW3Y/xvr+++9VqlQpTZ8+XefPn9f58+c1ffp0lS5dWt9//70zagQAAMgxu3t2oqOj1alTJ82aNUuenp6SpJs3b+qFF15QdHS09uzZ4/AiAQAAcsrunp2EhAS9+OKL1qAjSZ6enoqNjVVCQoJDiwMAAMgtu8POgw8+aB2r81f79+9X9erVHVIUAACAo9zRY6zdu3db/zxgwAANHDhQCQkJql+/viRp8+bNmjlzpt544w3nVAnArfEuHgDu7I7CTo0aNWSxWGQYhnXb0KFDM7Xr2rWrOnXq5LjqAAAAcumOws7Ro0edXQcAAIBT3FHYCQ8Pd3YdAIB7HI9D4Sw5eqlgYmKifvzxR50+fVoZGRk2+wYMGOCQwgAAABzB7rAzb948/fvf/5a3t7cKFy4si8Vi3WexWAg7AADArdgddkaNGqXRo0drxIgR8vCwe+Y6AADAXWV32Lly5Yo6d+5M0AGALPx13AljTvI2/i7Nw+7E0qdPH33xxRfOqAUA3Fqp4SusC4C8w+6enYkTJ6pNmzZauXKlqlatKi8vL5v9U6ZMcVhxAAC4M3p/8oYchZ1Vq1apQoUKkpRpgDIAAIA7sTvsTJ48WXPmzFGvXr2cUA4AAIBj2R12fHx81KhRI2fUAgD3BB59AHeX3QOUBw4cqBkzZjijFgAAAIezu2dn69atWr9+vZYvX67KlStnGqC8dOlShxUHAACQW3b37AQGBqp9+/Zq0qSJihQpooCAAJvFHt9//73atm2r0NBQWSwWLVu2zGa/YRgaPXq0ihcvLj8/PzVr1kyHDx+2aXPu3Dl169ZN/v7+CgwMVJ8+fXTp0iV7LwsAAJiU3T07c+fOddjJL1++rOrVq+uZZ55R+/btM+2fNGmSpk+frvnz56t06dIaNWqUoqKi9Msvv8jX11eS1K1bN506dUpr1qzRjRs31Lt3b/Xr108LFy50WJ0AALgSX5KaOzn6IlBHadmypVq2bJnlPsMwNG3aNL3yyit64oknJEkfffSRgoODtWzZMnXu3Fn79+/XypUrtW3bNtWuXVuSNGPGDLVq1Upvv/22QkND79q1AAAA92R32ClduvRt36fz66+/5qqgW44ePaqkpCQ1a9bMui0gIED16tVTXFycOnfurLi4OAUGBlqDjiQ1a9ZMHh4e2rJli5588sksj52Wlqa0tDTrempqqkNqBgAA7sfusDNo0CCb9Rs3bujnn3/WypUrNWTIEEfVpaSkJElScHCwzfbg4GDrvqSkJBUrVsxmf758+RQUFGRtk5WJEydq7NixDqsVAAC4L7vDzsCBA7PcPnPmTG3fvj3XBd0NI0aMUGxsrHU9NTVVYWFhLqwIAAA4i8O+urxly5ZasmSJow6nkJAQSVJycrLN9uTkZOu+kJAQnT592mZ/enq6zp07Z22TFR8fH/n7+9ssAADAnBwWdhYvXqygoCBHHU6lS5dWSEiI1q1bZ92WmpqqLVu2qEGDBpKkBg0a6MKFC4qPj7e2Wb9+vTIyMlSvXj2H1QIAAPIuux9j1axZ02aAsmEYSkpK0pkzZ/Tee+/ZdaxLly4pISHBun706FHt3LlTQUFBKlmypAYNGqTx48erfPny1qnnoaGhateunSSpUqVKatGihfr27avZs2frxo0biomJUefOnZmJBQAAJOUg7NwKGrd4eHioaNGiatq0qSpWrGjXsbZv365HHnnEun5rHE3Pnj01b948DR06VJcvX1a/fv104cIFNW7cWCtXrrS+Y0eSFixYoJiYGEVGRsrDw0MdOnTQ9OnT7b0s/H+8ywEAYDZ2h50xY8Y47ORNmzaVYRjZ7rdYLBo3bpzGjRuXbZugoCBeIAgAALLlsDE7AAAA7uiOe3Y8PDxu+zJB6c+emPT09FwXBQAA4Ch3HHa+/PLLbPfFxcVp+vTpysjIcEhRcAzG3wD3lr/+m+ffO/B/7jjs3Pp+qr86ePCghg8frv/973/q1q3bbcfWAAAAuEKOvgg0MTFRY8aM0fz58xUVFaWdO3eqSpUqjq4NcBh+4wWAe5ddYSclJUUTJkzQjBkzVKNGDa1bt04PPfSQs2oDTIGghXsBP+dwZ3ccdiZNmqQ333xTISEh+vTTT7N8rAUAAOBu7jjsDB8+XH5+fipXrpzmz5+v+fPnZ9lu6dKlDisOAAAgt+447PTo0eMfp54DAAC4mzsOO/PmzXNiGQAAAM7BG5QBAICp5WjqOYC8gRky2ePeAPcOwg5chjc8AwDuBsIO/tHd+g3YDOHHDNdgRvy9APc2xuwAAABTI+wAAABTI+wAAABTY8wOYBKMS8Hdws+aY3E/nY+eHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGrMxgJgw52+M4pZKgAcgZ4dAABgaoQdAABgajzGAoAs8AgNMA/CDgAAd4AAnHfxGAsAAJgaYQcAAJgaYQcAAJgaYQcAAJgaA5SRpzBAEMC9wJ1e7mkG9OwAAABTI+wAAABTI+wAAABTI+wAAABTI+wAAABTYzaWC+RklD0j83EvMOPPOTMIAdcj7MB07ub/XMz4P2fg7/g5R17HYywAAGBq9OwAgOi9AMyMnh0AAGBq9OwAboBeBTgDg6PvPu65e6JnBwAAmBo9O26I3wwAAHAcwg7sRhgDj90A5CWEHeAeQlAFcC9izA4AADA1enYAIA9w1qNDevtwLyDsAACsCD95B2Pn7hyPsQAAgKkRdgAAgKnxGAsOcTfGE9BNC/wfHjcBd46wAyBPIQADsBePsQAAgKnRswOnoIsdd4sZftbMcA2AO3Prnp1XX31VFovFZqlYsaJ1/7Vr1xQdHa3ChQurYMGC6tChg5KTk11YMQAAcDdu37NTuXJlrV271rqeL9//lTx48GCtWLFCX3zxhQICAhQTE6P27dtr06ZNrig1T2C8Q97A3xMAOI7bh518+fIpJCQk0/aUlBR9+OGHWrhwoR599FFJ0ty5c1WpUiVt3rxZ9evXv9ulAgAAN+TWj7Ek6fDhwwoNDVWZMmXUrVs3nThxQpIUHx+vGzduqFmzZta2FStWVMmSJRUXF3fbY6alpSk1NdVmAQAA5uTWPTv16tXTvHnzVKFCBZ06dUpjx47VQw89pL179yopKUne3t4KDAy0+UxwcLCSkpJue9yJEydq7NixTqwcZsCjJAAwB7cOOy1btrT+uVq1aqpXr57Cw8O1aNEi+fn55fi4I0aMUGxsrHU9NTVVYWFhuao1O3cyy+LvbczK3cODu9cHIGv828U/cfvHWH8VGBioBx54QAkJCQoJCdH169d14cIFmzbJyclZjvH5Kx8fH/n7+9ssAADAnNy6Z+fvLl26pCNHjqh79+6qVauWvLy8tG7dOnXo0EGSdPDgQZ04cUINGjRwcaW4m3LyWx3vNQGAe4dbh52XXnpJbdu2VXh4uBITEzVmzBh5enqqS5cuCggIUJ8+fRQbG6ugoCD5+/urf//+atCgATOxAACAlVuHnd9++01dunTR2bNnVbRoUTVu3FibN29W0aJFJUlTp06Vh4eHOnTooLS0NEVFRem9995zcdUAAMCduHXY+eyzz26739fXVzNnztTMmTPvUkW4lznq0VdOjsNjNwDIObcOOwCy56gZKMxkubfw9417UZ6ajQUAAGAvwg4AADA1HmPd4+6VFxoCAO5dhJ08igGryGv4mQXgKoQdE2HgIXB3OSvAEQyRE1n93PD/hT8xZgcAAJgaPTsAANwj/mmcZla9QXfSY+TuvUr07AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMzTdiZOXOmSpUqJV9fX9WrV09bt251dUkAAMANmCLsfP7554qNjdWYMWO0Y8cOVa9eXVFRUTp9+rSrSwMAAC5mirAzZcoU9e3bV71791ZERIRmz56t/Pnza86cOa4uDQAAuFg+VxeQW9evX1d8fLxGjBhh3ebh4aFmzZopLi4uy8+kpaUpLS3Nup6SkiJJSk1NdXh9GWlXbNZTU1Nttv19PSt38hlHtblXz/1PbTg3575X/i1wbs7trHM7w63jGoZx+4ZGHvf7778bkoyffvrJZvuQIUOMunXrZvmZMWPGGJJYWFhYWFhYTLCcPHnytlkhz/fs5MSIESMUGxtrXc/IyNC5c+dUuHBhWSyWXB8/NTVVYWFhOnnypPz9/XN9PNji/jof99i5uL/Oxz12Lne5v4Zh6OLFiwoNDb1tuzwfdooUKSJPT08lJyfbbE9OTlZISEiWn/Hx8ZGPj4/NtsDAQIfX5u/vzz8yJ+L+Oh/32Lm4v87HPXYud7i/AQEB/9gmzw9Q9vb2Vq1atbRu3TrrtoyMDK1bt04NGjRwYWUAAMAd5PmeHUmKjY1Vz549Vbt2bdWtW1fTpk3T5cuX1bt3b1eXBgAAXMwUYadTp046c+aMRo8eraSkJNWoUUMrV65UcHCwS+rx8fHRmDFjMj0qg2Nwf52Pe+xc3F/n4x47V167vxbD+Kf5WgAAAHlXnh+zAwAAcDuEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqmmHruSn/88YfmzJmjuLg4JSUlSZJCQkLUsGFD9erVS0WLFnVxhQAA3NuYep4L27ZtU1RUlPLnz69mzZpZ3+uTnJysdevW6cqVK1q1apVq167t4krzvq1bt2YKlA0aNFDdunVdXJk5cH+d6/r161q2bFmWvxQ98cQT8vb2dnGFeR/3GLdD2MmF+vXrq3r16po9e3amLxA1DEPPPfecdu/erbi4OBdVmPedPn1aHTp00KZNm1SyZEmbQHnixAk1atRIS5YsUbFixVxcad7E/XW+hIQERUVFKTExUfXq1bO5x1u2bNH999+vb7/9VuXKlXNxpXkX9/juSUpK0pYtW2wCZb169bL9Lkp3QdjJBT8/P/3888+qWLFilvsPHDigmjVr6urVq3e5MvPo2LGjEhMTNXfuXFWoUMFm38GDB/XMM88oNDRUX3zxhYsqzNu4v8732GOPqUCBAvroo48yfWFiamqqevTooatXr2rVqlUuqjDv4x473+XLl/Xvf/9bn332mSwWi4KCgiRJ586dk2EY6tKli/7zn/8of/78Lq40GwZyrFSpUsb8+fOz3T9//nwjPDz87hVkQgULFjR27NiR7f7t27cbBQsWvIsVmQv31/n8/PyMPXv2ZLt/9+7dhp+f312syHy4x87Xp08fo3z58sbKlSuN9PR06/b09HRj1apVxgMPPGA8++yzLqzw9hignAsvvfSS+vXrp/j4eEVGRmYas/PBBx/o7bffdnGVeZuPj49SU1Oz3X/x4sU8890s7oj763yBgYE6duyYqlSpkuX+Y8eOKTAw8O4WZTLcY+dbsmSJVqxYoYYNG9ps9/T0VPPmzTVnzhy1adNGH3zwgYsqvD3CTi5ER0erSJEimjp1qt577z3dvHlT0p9/+bVq1dK8efP01FNPubjKvK1Tp07q2bOnpk6dqsjISGsXdWpqqtatW6fY2Fh16dLFxVXmXdxf53v22WfVo0cPjRo1KstfisaPH6/+/fu7uMq8jXvsfBkZGbcd5O3t7a2MjIy7WJGdXN21ZBbXr183EhMTjcTEROP69euuLsc0rl27Zjz33HOGt7e34eHhYfj6+hq+vr6Gh4eH4e3tbTz//PPGtWvXXF1mnpXd/bVYLNxfB3rjjTeM4sWLGxaLxfDw8DA8PDwMi8ViFC9e3HjzzTddXZ4pcI+dq2vXrkbNmjWzfOy9Y8cOo1atWka3bt1cUNmdYYAy8oTU1FTFx8fbzACoVatWpsGIyJnU1FRt375dycnJkqTg4GDVrl2b++tgR48etfkZLl26tIsrMh/usXOcP39eXbt21apVq3TfffdZZ2iePn1aFy5cUFRUlBYuXOi2jwsJOwAy8fb21q5du1SpUiVXlwLAjezfv1+bN2/O9E6u7GYluwvCDtze1atXFR8fr6CgIEVERNjsu3btmhYtWqQePXq4qLq8LTY2Nsvt77zzjp5++mkVLlxYkjRlypS7WZap7NixQ/fdd5+1h+Hjjz/W7NmzdeLECYWHhysmJkadO3d2cZV537vvvqutW7eqVatW6ty5sz7++GNNnDhRGRkZat++vcaNG6d8+Rimeq/ibx5u7dChQ2revLlOnDghi8Wixo0b69NPP1VoaKgkKSUlRb179ybs5NC0adNUvXr1TF3PhmFo//79KlCgQKYXZsI+vXv31uTJk1W6dGn997//1YABA9S3b191795dBw8eVN++fXXlyhU988wzri41zxo/frwmTZqk5s2ba/DgwTp+/LjeeustDR48WB4eHpo6daq8vLw0duxYV5eap+Xpt1S7cLwQ8I/atWtntG7d2jhz5oxx+PBho3Xr1kbp0qWN48ePG4ZhGElJSYaHh4eLq8y7Jk6caJQuXdpYt26dzfZ8+fIZ+/btc1FV5uLn52ccO3bMMAzDqFmzpvH+++/b7F+wYIERERHhitJMo2zZssaSJUsMwzCMnTt3Gp6ensYnn3xi3b906VKjXLlyrirPFA4fPmyUKVPG8PX1NZo0aWI89dRTxlNPPWU0adLE8PX1NcqVK2ccPnzY1WVmi8dYcGvBwcFau3atqlatKunPHocXXnhB33zzjTZs2KACBQooNDTUOu0f9tu2bZuefvpptW3bVhMnTpSXl5e8vLy0a9euTI8NYb8iRYpo1apVqlWrloKDg7V69WpVr17duv/IkSOqWrWqrly54sIq87b8+fPrwIEDKlmypKQ/x5z9/PPPqly5siTp+PHjioiI0OXLl11ZZp6W199S7eHqAoDbuXr1qs1zdovFolmzZqlt27Zq0qSJDh065MLqzKFOnTqKj4/XmTNnVLt2be3du5dHVw7UsmVLzZo1S5LUpEkTLV682Gb/okWL+M6mXAoJCdEvv/wiSTp8+LBu3rxpXZekffv28f1uubRp0yaNHz8+yxma/v7+eu211/TDDz+4oLI7w5gduLWKFStq+/btmWYFvfvuu5Kkxx9/3BVlmU7BggU1f/58ffbZZ2rWrBk9ZQ705ptvqlGjRmrSpIlq166tyZMna+PGjapUqZIOHjyozZs368svv3R1mXlat27d1KNHDz3xxBNat26dhg4dqpdeeklnz56VxWLR66+/ro4dO7q6zDwtz7+l2sWP0YDbmjBhgtGyZcts9z///POGxWK5ixWZ38mTJ41ly5YZly5dcnUppnH+/Hlj2LBhRkREhOHr62t4e3sb4eHhRteuXY1t27a5urw87+bNm8brr79utGnTxpgwYYKRkZFhfPrpp0ZYWJhRuHBho1evXvw859KoUaOM++67z5gyZYqxa9cuIykpyUhKSjJ27dplTJkyxQgKCjLGjBnj6jKzxZgdAADwj95880298847SkpKsj7qNgxDISEhGjRokIYOHeriCrNH2AEAAHcsL76lmrADAABy5eTJkxozZozmzJnj6lKyRNgBAAC5smvXLj344INuO7mB2VgAAOC2vv7669vu//XXX+9SJTlDzw4AALgtDw8PWSwW3S4yWCwWt+3Z4aWCAADgtooXL66lS5cqIyMjy2XHjh2uLvG2CDsAAOC2atWqpfj4+Gz3/1Ovj6sxZgcAANzWkCFDbvvdYuXKldOGDRvuYkX2YcwOAAAwNR5jAQAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsADCNefPmKTAwMNfHsVgsWrZsWa6PA8A9EHYAuJVevXqpXbt2ri4DgIkQdgAAgKkRdgDkGVOmTFHVqlVVoEABhYWF6YUXXtClS5cytVu2bJnKly8vX19fRUVF6eTJkzb7v/rqKz344IPy9fVVmTJlNHbsWKWnp2d5zuvXrysmJkbFixeXr6+vwsPDNXHiRKdcHwDnIOwAyDM8PDw0ffp07du3T/Pnz9f69es1dOhQmzZXrlzR66+/ro8++kibNm3ShQsX1LlzZ+v+H374QT169NDAgQP1yy+/6D//+Y/mzZun119/PctzTp8+XV9//bUWLVqkgwcPasGCBSpVqpQzLxOAg/EGZQBupVevXrpw4cIdDRBevHixnnvuOf3xxx+S/hyg3Lt3b23evFn16tWTJB04cECVKlXSli1bVLduXTVr1kyRkZEaMWKE9TiffPKJhg4dqsTEREl/DlD+8ssv1a5dOw0YMED79u3T2rVrZbFYHH/BAJyOnh0AecbatWsVGRmpEiVKqFChQurevbvOnj2rK1euWNvky5dPderUsa5XrFhRgYGB2r9/vyRp165dGjdunAoWLGhd+vbtq1OnTtkc55ZevXpp586dqlChggYMGKDVq1c7/0IBOBRhB0CecOzYMbVp00bVqlXTkiVLFB8fr5kzZ0r6c1zNnbp06ZLGjh2rnTt3Wpc9e/bo8OHD8vX1zdT+wQcf1NGjR/Xaa6/p6tWreuqpp9SxY0eHXRcA5+NbzwHkCfHx8crIyNDkyZPl4fHn72mLFi3K1C49PV3bt29X3bp1JUkHDx7UhQsXVKlSJUl/hpeDBw+qXLlyd3xuf39/derUSZ06dVLHjh3VokULnTt3TkFBQQ64MgDORtgB4HZSUlK0c+dOm21FihTRjRs3NGPGDLVt21abNm3S7NmzM33Wy8tL/fv31/Tp05UvXz7FxMSofv361vAzevRotWnTRiVLllTHjh3l4eGhXbt2ae/evRo/fnym402ZMkXFixdXzZo15eHhoS+++EIhISEOeXkhgLuDx1gA3M7GjRtVs2ZNm+Xjjz/WlClT9Oabb6pKlSpasGBBllPA8+fPr2HDhqlr165q1KiRChYsqM8//9y6PyoqSsuXL9fq1atVp04d1a9fX1OnTlV4eHiWtRQqVEiTJk1S7dq1VadOHR07dkzffPONtXcJgPtjNhYAADA1fjUBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACm9v8Az02ubZG9wikAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "\n",
        "mat = scipy.io.loadmat(labels.rsplit('/', 1)[-1])\n",
        "y = pd.Series(mat['labels'][0])\n",
        "unique_labels = y.unique()\n",
        "print(f\"Number of unique classes: {len(unique_labels)}\")\n",
        "\n",
        "plt.bar(unique_labels, y.value_counts())\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.title('Label Distribution')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOf48brIkdrl"
      },
      "source": [
        "CNN off-the-shelf\n",
        "\n",
        "\n",
        "*   Take pre-trained CNN (YOLOV5, VGG19)\n",
        "*   Removed the last fully-connected layer\n",
        "*   Take the rest of the CNN as a fixed feature extractor\n",
        "*   Train on https://www.robots.ox.ac.uk/~vgg/data/flowers/102/ \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SJTYAUY-dJRp"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.applications import vgg19\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # load_img, img_to_array, array_to_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTGl1PGycB29"
      },
      "source": [
        "Extract the images and prepare to read them using ImageDataGenerator - "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y-5UAxKbUM4G"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "tgz_file = dataset.rsplit('/', 1)[-1]\n",
        "\n",
        "with tarfile.open(tgz_file, 'r:gz') as file:\n",
        "    # Extract all files to the specified directory\n",
        "    file.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODOS:\n",
        "1. Getting it to work (VGG19 + flow_from_dataframe) -\n",
        "    1. test current setup with GPU\n",
        "    2. try to fix one-hot encoding\n",
        "    3. try transfer learning with less layers (e.g. layer 4)\n",
        "\n",
        "2. Hyperparameter Tuning\n",
        "3. Write code blocks in functions and apply to YOLOv5 also\n",
        "4. Provide an accuracy graph and the Cross-Entropy graph for train/validation/test as a function of the number of epochs for all models."
      ],
      "metadata": {
        "id": "ON0wxlGABQ7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code block currently fails due to RAM issues\n",
        "\n",
        "# # Load images to array (data generator \"flow\")\n",
        "# image_path = \"./jpg\"\n",
        "# image_file_names = os.listdir(image_path)\n",
        "# image_file_names.sort()\n",
        "# image_array_list = []\n",
        "# for image_file_name in image_file_names:\n",
        "#     tf_image = keras.preprocessing.image.load_img(\n",
        "#         path=f\"{image_path}/{image_file_name}\",\n",
        "#         grayscale=False,\n",
        "#         target_size=(224,224,),\n",
        "#     )\n",
        "#     img_array = keras.preprocessing.image.img_to_array(tf_image)\n",
        "#     process_img = vgg19.preprocess_input(img_array)\n",
        "#     image_array_list.append(process_img)\n",
        "\n",
        "# # Prepare train/validation/test sets (0.5/0.25/0.25)\n",
        "# X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
        "#     image_array_list,\n",
        "#     one_hot_labels,\n",
        "#     test_size=0.5,\n",
        "#     random_state=seed,\n",
        "#     shuffle=True,\n",
        "#     stratify=y,\n",
        "# )\n",
        "# X_val, X_test, y_val, y_test = train_test_split(\n",
        "#     X_val_test,\n",
        "#     y_val_test,\n",
        "#     test_size=0.5,\n",
        "#     random_state=seed,\n",
        "#     shuffle=True,\n",
        "#     stratify=y_val_test,\n",
        "# )\n",
        "\n",
        "# # Prepare data generators\n",
        "# train_data_gen = ImageDataGenerator()\n",
        "# valid_data_gen = ImageDataGenerator()\n",
        "# test_data_gen = ImageDataGenerator()\n",
        "\n",
        "# # Setting up the generators\n",
        "# training_generator = train_data_gen.flow(\n",
        "#     x=np.array(X_train), y=y_train, batch_size=32\n",
        "# )\n",
        "# validation_generator = valid_data_gen.flow(\n",
        "#     x=np.array(X_val), y=y_val, batch_size=32\n",
        "# )\n",
        "# test_generator = test_data_gen.flow(\n",
        "#     x=np.array(X_test), y=y_test, batch_size=32\n",
        "# )"
      ],
      "metadata": {
        "id": "EVokNngvqC1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# OneHotEncoding for labels\n",
        "oh_encoder = OneHotEncoder()\n",
        "\n",
        "image_labels = scipy.io.loadmat(labels.rsplit('/', 1)[-1])[\"labels\"][0]\n",
        "image_labels_2d = image_labels.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "one_hot_labels = encoder.fit_transform(image_labels_2d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIXRLOmOfA8o",
        "outputId": "75130efb-77c5-43df-961b-25d156116e18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t1XEhRapX3h3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "dad08782-d5ec-4a72-a1c4-0c64e894d729"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          filename class\n",
              "0  image_06966.jpg    77\n",
              "1  image_07188.jpg    77\n",
              "2  image_03578.jpg    77\n",
              "3  image_01030.jpg    77\n",
              "4  image_06418.jpg    77"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c5269e7-ff18-4742-b0ae-6c5094e808fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image_06966.jpg</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image_07188.jpg</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image_03578.jpg</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image_01030.jpg</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image_06418.jpg</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c5269e7-ff18-4742-b0ae-6c5094e808fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c5269e7-ff18-4742-b0ae-6c5094e808fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c5269e7-ff18-4742-b0ae-6c5094e808fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Prepare dataframe for flow_from_dataframe\n",
        "data_dir = os.path.join(os.getcwd(), 'jpg')\n",
        "df = pd.DataFrame({\"filename\": os.listdir(data_dir),  \"class\": y.astype(str)})\n",
        "df.head()\n",
        "\n",
        "# This might be needed for the onehotencoded labels?\n",
        "# df['class'] = df['class'].apply(lambda row: row.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0RFsA5BcJ2p"
      },
      "source": [
        "Split the data to Training (50%), Validation (25%) and Test (25%) sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lg9FZo29RyMK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preprocessing - define image size required by the model\n",
        "# Define preprocessing function as the models preprocess_input function\n",
        "\n",
        "def get_data_generators(df, img_size, preprocessing_function, batch_size=32):\n",
        "    train_df, val_test_df = train_test_split(df, stratify=df['class'], test_size=0.5, \n",
        "                                             random_state=seed, shuffle=True)\n",
        "    val_df, test_df = train_test_split(val_test_df, \n",
        "                                       stratify=val_test_df['class'], \n",
        "                                       test_size=0.5, random_state=seed, shuffle=True)\n",
        "\n",
        "    # TODO: Need to check if resizing helps\n",
        "    train_data_gen = ImageDataGenerator(preprocessing_function=preprocessing_function)#, resize=1/255)\n",
        "    valid_data_gen = ImageDataGenerator(preprocessing_function=preprocessing_function)#, resize=1/255)\n",
        "    test_data_gen = ImageDataGenerator(preprocessing_function=preprocessing_function)#, resize=1/255)\n",
        "    print(\"Training:\")\n",
        "    train_generator = train_data_gen.flow_from_dataframe(\n",
        "        train_df,\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    print(\"Validation:\")\n",
        "    validation_generator = valid_data_gen.flow_from_dataframe(\n",
        "        val_df,\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    print(\"Test:\")\n",
        "    test_generator = test_data_gen.flow_from_dataframe(\n",
        "        test_df,\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator, test_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jimWRpId5BIj"
      },
      "source": [
        "From keras guide to transfer learning -\n",
        "1.   Take layers from a previously trained model.\n",
        "2.  Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n",
        "3. Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n",
        "4. Train the new layers on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usYlWkr-htOK"
      },
      "source": [
        "### VGG19 - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbLafR_D5bgp"
      },
      "source": [
        "Preprocessing for this model is accomplished with the preprocess_input function which converts the input images from RGB to BGR and zero-centers each color channel with respect to the ImageNet dataset, without scaling the image. The images are read with target_size 224x224 which is the input side of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0Ap-TzU6jG89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d527ef-1b24-4fab-d484-f03e2e41d832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:\n",
            "Found 4094 validated image filenames belonging to 102 classes.\n",
            "Validation:\n",
            "Found 2047 validated image filenames belonging to 102 classes.\n",
            "Test:\n",
            "Found 2048 validated image filenames belonging to 102 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator, validation_generator, test_generator = get_data_generators(df, img_size=(224, 224), preprocessing_function=vgg19.preprocess_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seJJiYrwjbJw"
      },
      "source": [
        "load the model (without its final top predictive layers) and take a look at its structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iDHBmPTF6ADr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6566261e-ad8b-4e0a-8a79-19418b989f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 0s 0us/step\n",
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Load the VGG19 model with its imagenet pretrained weights\n",
        "# but without the final 3 fully connected layers at the top of the network\n",
        "base_model = vgg19.VGG19(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0wbNDTN8S5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5200463-e363-48d3-9af2-064188f9bada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "128/128 [==============================] - 3030s 24s/step - loss: 6.5043 - accuracy: 0.0259 - val_loss: 4.6147 - val_accuracy: 0.0313\n",
            "Epoch 2/50\n",
            " 92/128 [====================>.........] - ETA: 9:25 - loss: 4.5510 - accuracy: 0.0418"
          ]
        }
      ],
      "source": [
        "# Set base_model weights to not be trainable (required for transfer learning)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new \"top model\" to meld with the base_model\n",
        "# Only top model will be trained in transfer learning\n",
        "inputs = keras.layers.Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = Flatten()(x)  # Add a Flatten layer here\n",
        "x = Dense(256, activation='relu')(x)  # Add a Dense layer for additional non-linearity\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(len(unique_labels), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Train the model on the flowers images\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=5, monitor='val_accuracy', mode=\"max\")\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=50, callbacks=[early_stopping], \n",
        "                    validation_data=validation_generator, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv423miX7xmz",
        "outputId": "70a2eb88-837c-4c2f-e4de-07c80040cbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 9.2068 - accuracy: 0.0129 "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "These cells are different ways to setup the \"top-model\"\n",
        "not really needed if the top part will work but can try different \"top-model\"\n",
        "architectures\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Transfer learning - Freeze the weights of the base_model \n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a new top model\n",
        "n_classes = len(y.unique())\n",
        "\n",
        "top_model = Sequential()\n",
        "top_model.add(keras.layers.GlobalAveragePooling2D())\n",
        "top_model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# Combine the base_model with new top model\n",
        "# bot_model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(top_model)\n",
        "\n",
        "# Train the model on the flowers images\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=100, callbacks=[early_stopping], \n",
        "                    validation_data=validation_generator, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFpipZo8Tm7D"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvHUIZD_kuw1"
      },
      "outputs": [],
      "source": [
        "# Transfer learning - Freeze the weights of the base_model \n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a new top model\n",
        "n_classes = len(y.unique())\n",
        "\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten())\n",
        "top_model.add(Dropout(rate=0.2))\n",
        "top_model.add(Dense(1024, activation='relu'))\n",
        "top_model.add(Dropout(rate=0.2))\n",
        "top_model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# Combine the base_model with new top model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(top_model)\n",
        "\n",
        "# Train the model on the flowers images\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=100, callbacks=[early_stopping], \n",
        "                    validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7b8Vamb52tt"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsalMDhJ58HZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYGvXoN5m6A9lPYLWbB/NT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}