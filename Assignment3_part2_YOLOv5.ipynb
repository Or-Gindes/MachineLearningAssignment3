{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9a5DNoPd-_h"
      },
      "source": [
        "# YOLOv5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgeI3rL5nSRP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import scipy.io\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import io\n",
        "import shutil\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS9jDknIgM4Z"
      },
      "outputs": [],
      "source": [
        "# clone YOLOv5 repository\n",
        "! git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "# install dependencies as necessary (ignore errors)\n",
        "! pip install -qr requirements.txt\n",
        "! pip install -U ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p3OJGMz29JZi"
      },
      "outputs": [],
      "source": [
        "dataset = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
        "labels = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
        "urlretrieve(dataset, dataset.rsplit('/', 1)[-1])\n",
        "urlretrieve(labels, labels.rsplit('/', 1)[-1])\n",
        "\n",
        "tgz_file = dataset.rsplit('/', 1)[-1]\n",
        "os.makedirs(\"/model2/classes/\")\n",
        "with tarfile.open(tgz_file, 'r:gz') as file:\n",
        "    # Extract all files to the specified directory\n",
        "    file.extractall('/model2/classes/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0GmmfjkfrLQ",
        "outputId": "dbe58b68-c1d0-4726-ebd3-c13fc4dd7ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes: 102\n"
          ]
        }
      ],
      "source": [
        "mat = scipy.io.loadmat(labels.rsplit('/', 1)[-1])\n",
        "y = pd.Series(mat['labels'][0])\n",
        "# Shift the index by one position\n",
        "y.index = y.index + 1\n",
        "unique_labels = y.unique()\n",
        "print(f\"Number of unique classes: {len(unique_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"filename\": sorted(os.listdir('/model2/classes/jpg')),  \"class\": y.astype(str)})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "sELKLQSSrJmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBZZbs0kV6Iq"
      },
      "source": [
        "To run the YOLOv5 model, the dataset needs to be organized in a specific way:\n",
        "\n",
        "Start with a base directory, which in our case is /dataset.\n",
        "Inside the base directory, create two subfolders: train and val. These folders will hold the training and validation images, respectively.\n",
        "Within each of the train and val folders, create subfolders for each classification class. In our case, there are 102 classes numbered from 1 to 102. Each class folder will contain images belonging to that specific class.\n",
        "\n",
        "To achieve this organization, the code performs the following steps:\n",
        "- Unpacking all the images to the /model2/classes directory.\n",
        "- Looping over a list that holds the class each image belongs to: This list provides the class label for each image.\n",
        "- Moving all the images to the /dataset directory: Each image is moved to the /dataset directory under the corresponding class subfolder, based on its class label. (e.g /dataset/71/image_07483.jpg)\n",
        "- Creating a list of all the images in the dataset: This list is created to facilitate the random splitting of the dataset.\n",
        "- Dividing the image array into three random arrays: The dataset is split into three arrays: train, validation, and test. These arrays will be used for training, validation, and testing the YOLOv5 model.\n",
        "- Creating the train and val subdirectories with the corresponding images: The train and val subdirectories are created within the /dataset directory. The images selected for training and validation are placed inside their respective subdirectories under the class subfolders."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_dataset():\n",
        "  for index, class_label in y.items():\n",
        "    # print(f'Index is {index} and class_label is {class_label}')\n",
        "    image_filename = f\"image_{index:05d}.jpg\"  # Assuming the image filenames follow the pattern image_#####.jpg\n",
        "    source_path = os.path.join('/model2/classes/jpg/', image_filename)\n",
        "    # print(f\"source path {source_path}\")\n",
        "    target_path = os.path.join('/dataset', str(class_label), image_filename)\n",
        "    # print(f\"target path {target_path}\")\n",
        "    os.makedirs(os.path.join('/dataset', str(class_label)), exist_ok=True)\n",
        "    shutil.copyfile(source_path, target_path)\n",
        "    # print(os.listdir(os.path.join('/dataset/', str(class_label))))\n",
        "\n",
        "def find_file_directory(file_name, directory):\n",
        "    file_path = None\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        if file_name in files:\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            break\n",
        "    if file_path:\n",
        "        file_directory = os.path.dirname(file_path)\n",
        "        return file_directory\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def move_images_to_directory(lst, source_dir, target_dir):\n",
        "    for image_filename in lst:\n",
        "        img_curr_path = os.path.join(find_file_directory(image_filename, '/dataset'), image_filename)\n",
        "        image_class = os.path.basename(os.path.dirname(img_curr_path))\n",
        "        target_path = os.path.join(target_dir, image_class, image_filename)\n",
        "        # Create the target directory with class folder if it doesn't exist\n",
        "        os.makedirs(os.path.join(target_dir, image_class), exist_ok=True)\n",
        "        shutil.move(img_curr_path, target_path)\n",
        "\n",
        "def copy_files_from_subdirectories(source_dir, target_dir):\n",
        "    # Create the target directory if it doesn't exist\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    # Iterate over all subdirectories in the source directory\n",
        "    for root, dirs, files in os.walk(source_dir):\n",
        "        for file in files:\n",
        "            # Get the full path of the file\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Copy the file to the target directory\n",
        "            shutil.copy2(file_path, target_dir)\n",
        "\n",
        "def count_files_in_directory(directory):\n",
        "    count = 0\n",
        "    for _, _, files in os.walk(directory):\n",
        "        count += len(files)\n",
        "    return count\n",
        "\n",
        "\n",
        "def sum_train_and_val_loss_per_epoch(i):\n",
        "  name = 'flower_classifier' if i == 0 else f'flower_classifier{i+1}'\n",
        "  train_loss_avg_epochs, val_loss_avg_epochs, val_acc_avg_epochs = {}, {}, {}\n",
        "  train_val_df = pd.read_csv(f'/content/yolov5/runs/train-cls/{name}/results.csv')\n",
        "  train_val_df.columns = train_val_df.columns.str.strip()\n",
        "\n",
        "  for _, row in train_val_df.iterrows():\n",
        "      epoch = int(row['epoch'])\n",
        "      train_loss = row['train/loss']\n",
        "      val_loss = row['val/loss']\n",
        "      acc = row['metrics/accuracy_top1']\n",
        "\n",
        "      if epoch not in train_loss_avg_epochs:\n",
        "          train_loss_avg_epochs[epoch] = train_loss\n",
        "      else:\n",
        "          train_loss_avg_epochs[epoch] += train_loss\n",
        "\n",
        "      if epoch not in val_loss_avg_epochs:\n",
        "          val_loss_avg_epochs[epoch] = val_loss\n",
        "      else:\n",
        "          val_loss_avg_epochs[epoch] += val_loss\n",
        "\n",
        "      if epoch not in val_acc_avg_epochs:\n",
        "        val_acc_avg_epochs[epoch] = acc\n",
        "      else:\n",
        "        val_acc_avg_epochs[epoch] += acc\n",
        "\n",
        "  return train_loss_avg_epochs, val_loss_avg_epochs, val_acc_avg_epochs\n",
        "\n",
        "def calc_train_val_loss_acc_avg(n, train_loss_avg_epochs, val_loss_avg_epochs, val_acc_avg_epochs):\n",
        "  for epoch in train_loss_avg_epochs:\n",
        "      train_loss_avg_epochs[epoch] /= n\n",
        "\n",
        "  for epoch in val_loss_avg_epochs:\n",
        "      val_loss_avg_epochs[epoch] /= n\n",
        "\n",
        "  for epoch in val_acc_avg_epochs:\n",
        "      val_acc_avg_epochs[epoch] /= n\n",
        "\n",
        "  return train_loss_avg_epochs, val_loss_avg_epochs, val_acc_avg_epochs\n",
        "\n",
        "def calculate_correct_pred(df_model, df_total):\n",
        "    correct = 0\n",
        "    for index, row in df_model.iterrows():\n",
        "      image = row['image'].rstrip(':')\n",
        "      max_class = row['max_class']\n",
        "      matching_row = df_total[df_total['filename'] == image]\n",
        "      if not matching_row.empty:\n",
        "        class_value = matching_row.iloc[0]['class']\n",
        "        # Compare class values\n",
        "        if max_class == class_value:\n",
        "            correct = correct +1\n",
        "    return correct\n",
        "\n",
        "def clean_runtime_env(to_del: list[str]):\n",
        "  for path in to_del:\n",
        "    if os.path.isdir(path):\n",
        "      print(f\"delete path - {path}\")\n",
        "      shutil.rmtree(path)"
      ],
      "metadata": {
        "id": "gYKwIzWx2Yz1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1OJv_M2KgTOo"
      },
      "outputs": [],
      "source": [
        "def train_and_eval(df, n, num_epoch, seed, optimizer,lr, dropout, batch_size):\n",
        "  # Delete old run folder if exists\n",
        "  if os.path.isdir(os.path.join(os.getcwd(), 'runs')):\n",
        "    shutil.rmtree(os.path.join(os.getcwd(), 'runs'))\n",
        "\n",
        "  test_accuracy = []\n",
        "\n",
        "  for i in range(n):\n",
        "    copy_dataset()\n",
        "    train_df, val_test_df = train_test_split(df, test_size=0.5, stratify=df['class'],  shuffle=True, random_state=seed[i])\n",
        "    val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['class'], shuffle=True, random_state=seed[i])\n",
        "\n",
        "    move_images_to_directory(train_df['filename'], '/dataset', '/dataset/train')\n",
        "    print(f\"Number of training images: {count_files_in_directory('/dataset/train')}\")\n",
        "    move_images_to_directory(val_df['filename'], '/dataset', '/dataset/val')\n",
        "    print(f\"Number of validation images: {count_files_in_directory('/dataset/val')}\")\n",
        "    move_images_to_directory(test_df['filename'], '/dataset', '/test_dataset')\n",
        "    print(f\"Number of testing images: {count_files_in_directory('/test_dataset')}\")\n",
        "    # /test_dataset is the test directory with the image classification and /test directory is a directory with all image tests files (not splitted into classes)\n",
        "    copy_files_from_subdirectories('/test_dataset', '/test')\n",
        "\n",
        "    # optimizer options ['SGD', 'Adam', 'AdamW', 'RMSProp']\n",
        "    !python classify/train.py --model yolov5s-cls.pt --data /dataset --epochs $num_epoch --pretrained yolov5s-cls.pt --name flower_classifier --lr0 $lr --seed \"${seed[i]}\" --batch-size $batch_size\n",
        "\n",
        "    train_loss_avg_epochs,val_loss_avg_epochs, val_acc_avg_epochs = sum_train_and_val_loss_per_epoch(i)\n",
        "\n",
        "    # calculate test score for iteration\n",
        "    # Run the predict command according to the weights from the train command\n",
        "    name = 'flower_classifier' if i == 0 else f'flower_classifier{i+1}'\n",
        "    weights_path = f\"runs/train-cls/{name}/weights/best.pt\"\n",
        "\n",
        "    output = !python classify/predict.py --weights $weights_path --source /test\n",
        "\n",
        "    # Process the output and store it in a list\n",
        "    data = []\n",
        "    for line in output:\n",
        "        if line.startswith('image'):\n",
        "            parts = line.split()\n",
        "            image_path = parts[2].replace('/test/', '').rstrip(':')\n",
        "            max_class = parts[4]\n",
        "            data.append({'image': image_path, 'max_class': max_class})\n",
        "\n",
        "    # Create a DataFrame from the list of dictionaries\n",
        "    df_model = pd.DataFrame(data)\n",
        "\n",
        "    # calculate the number of correct predictions\n",
        "    correct_pred = calculate_correct_pred(df_model, df)\n",
        "    # number of total test samples\n",
        "    total_test_samples = df_model.shape[0]\n",
        "    accuracy = correct_pred/total_test_samples\n",
        "    print(f\"Test accuracy cycle {i}: {round(accuracy * 100, 3)}%\")\n",
        "    test_accuracy.append(accuracy)\n",
        "    clean_runtime_env(['/dataset', '/test', '/test_dataset'])\n",
        "\n",
        "  print(f\"model accuracy = {round(np.mean(test_accuracy) * 100, 3)}%\")\n",
        "  #return the avg per epoch\n",
        "\n",
        "  return calc_train_val_loss_acc_avg(n, train_loss_avg_epochs, val_loss_avg_epochs, val_acc_avg_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "train_loss_avg, val_loss_avg, val_acc_avg = train_and_eval(df, n=2, num_epoch=50, seed=[123, 42], optimizer='ADAM', lr=0.001, dropout=0, batch_size=64)\n",
        "print(f'train loss {train_loss_avg}')\n",
        "print(f'val loss {val_loss_avg}')\n",
        "print(f'val acc {val_acc_avg}')"
      ],
      "metadata": {
        "id": "spq94HXkMsfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(list(train_loss_avg.keys())) + 1, list(train_loss_avg.values()), label='Training')\n",
        "plt.plot(np.array(list(val_loss_avg.keys())) + 1, list(val_loss_avg.values()), label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.array(list(val_acc_avg.keys())) + 1, list(val_acc_avg.values()), label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R5nP-c0n19yd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}